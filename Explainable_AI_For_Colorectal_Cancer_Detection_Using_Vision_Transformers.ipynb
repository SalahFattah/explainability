{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOteFSz02PA8rsATTwbaNdl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SalahFattah/explainability/blob/main/Explainable_AI_For_Colorectal_Cancer_Detection_Using_Vision_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMYwebNNDIpr"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "bWC9R1Q0Euy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dataset**"
      ],
      "metadata": {
        "id": "Dn1-cxuTGi2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_paths(data_root):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    label_names = sorted([\n",
        "        name for name in os.listdir(data_root)\n",
        "        if os.path.isdir(os.path.join(data_root, name)) and not name.startswith('.')\n",
        "    ])\n",
        "\n",
        "    label2id = {label: idx for idx, label in enumerate(label_names)}\n",
        "\n",
        "    for label in label_names:\n",
        "        class_dir = os.path.join(data_root, label)\n",
        "        # for img_path in glob.glob(os.path.join(class_dir, '*.jpg')):\n",
        "        for img_path in glob.glob(os.path.join(class_dir, '*.tif')):\n",
        "            image_paths.append(img_path)\n",
        "            labels.append(label2id[label])\n",
        "\n",
        "    return pd.DataFrame({\"image_path\": image_paths, \"label\": labels}), label2id\n"
      ],
      "metadata": {
        "id": "lGa7QPLBEvmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = './Colorectal_Histology/Kather_texture_2016_image_tiles_5000/Kather_texture_2016_image_tiles_5000'\n",
        "# data_root = './kvasir-dataset'"
      ],
      "metadata": {
        "id": "w6LkcA39EzFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, label2id = load_image_paths(data_root)"
      ],
      "metadata": {
        "id": "jxhFp21tE1J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id"
      ],
      "metadata": {
        "id": "6AL4KRVQE3Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['label'], random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n"
      ],
      "metadata": {
        "id": "mdLU6ATlE463"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "ds = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
        "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
        "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
        "})\n"
      ],
      "metadata": {
        "id": "3joVZAoDE7cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "NkPN2bmgE-IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "-u4pPPG2E_x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8hgOJn2JE-mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/vit-base-patch16-224\"                        # the model name\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)   # load the image processor\n",
        "model = ViTForImageClassification.from_pretrained(model_name, output_attentions=True)# loading the pre-trained model"
      ],
      "metadata": {
        "id": "zkdKRqabFKa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "4-n7a4vEFNiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(examples):\n",
        "  # convert all images to RGB format, then preprocessing it using our image processor\n",
        "  inputs = image_processor([Image.open(img).convert(\"RGB\") for img in examples[\"image_path\"]], return_tensors=\"pt\")\n",
        "\n",
        "  return {\"pixel_values\": inputs[\"pixel_values\"],\"labels\":examples[\"label\"]}\n",
        "\n"
      ],
      "metadata": {
        "id": "Gz0202X2FSKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ds.with_transform(transform)"
      ],
      "metadata": {
        "id": "bd_07iN2GwQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]['pixel_values'].shape"
      ],
      "metadata": {
        "id": "8qWQ4QE3Gygp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def show_image(dataset, split, index):\n",
        "    data = dataset[split][index]\n",
        "    image_tensor = data['pixel_values']\n",
        "    label = data['labels']\n",
        "\n",
        "    # Convert the tensor to a NumPy array and display the image\n",
        "    image_array = image_tensor.numpy()  # Convert the torch tensor to a NumPy array\n",
        "    print(image_array.shape)\n",
        "    # plt.figure(figsize=(1, 1))\n",
        "    plt.imshow(image_array.transpose(1, 2, 0))  # Matplotlib expects the channel dimension last\n",
        "    plt.title(f\"Class {label}\", fontsize=8)\n",
        "    plt.axis('off')  # Hide the axis\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qwvzEgkPG2jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.image\n",
        "idx = 7\n",
        "\n",
        "show_image(dataset, \"train\", idx)"
      ],
      "metadata": {
        "id": "8L-IlsjEIQi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=list(label2id.keys())\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "1MoAFf0-ITXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }\n"
      ],
      "metadata": {
        "id": "dBx0H87uIVrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "# load the accuracy and f1 metrics from the evaluate module\n",
        "accuracy = load(\"accuracy\")\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  # compute the accuracy and f1 scores & return them\n",
        "  accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
        "  f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
        "  return {**accuracy_score, **f1_score}"
      ],
      "metadata": {
        "id": "t3gUWm2ZIXlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(labels),\n",
        "    id2label={str(i): c for i, c in enumerate(labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "metadata": {
        "id": "DtYI8bp-IaZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "import transformers"
      ],
      "metadata": {
        "id": "OSpFUsz3Ic5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the training arguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./vit-base-colerctal\", # output directory\n",
        "  per_device_train_batch_size=16, # batch size per device during training\n",
        "  eval_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=10,             # total number of training epochs\n",
        "  # fp16=True,                    # use mixed precision\n",
        "  save_steps=374,                # number of update steps before saving checkpoint\n",
        "  eval_steps=374,                # number of update steps before evaluating\n",
        "  logging_steps=374,             # number of update steps before logging\n",
        "  save_total_limit=2,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,              # do not push the model to the hub\n",
        "  report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,    # load the best model at the end of training\n",
        ")"
      ],
      "metadata": {
        "id": "OQa7P0CkIkwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Trainer\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                        # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                 # training arguments, defined above\n",
        "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "    train_dataset=dataset[\"train\"],     # training dataset\n",
        "    eval_dataset=dataset[\"validation\"], # evaluation dataset\n",
        "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        ")"
      ],
      "metadata": {
        "id": "E260iAffIr9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "K0Co2XvhIuZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(dataset[\"test\"])"
      ],
      "metadata": {
        "id": "bNMSwgmeIwGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "outputs = trainer.predict(dataset[\"test\"])\n",
        "y_true = outputs.label_ids\n",
        "y_pred = outputs.predictions.argmax(1)\n",
        "\n",
        "# labels = dataset[\"train\"].features['label'].names\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(xticks_rotation=45)"
      ],
      "metadata": {
        "id": "rcnty5JNIyNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, f1_score, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "precision, recall, f1_score, support"
      ],
      "metadata": {
        "id": "RhsAYvMqIz5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = trainer.state.log_history\n",
        "# Extracting data for plotting\n",
        "epochs = [entry['epoch'] for entry in data]  # Extract epoch values\n",
        "train_losses = [entry['loss'] for entry in data if 'loss' in entry]  # Extract training losses\n",
        "eval_losses = [entry['eval_loss'] for entry in data if 'eval_loss' in entry]  # Extract evaluation losses\n",
        "\n",
        "# Plotting training and evaluation metrics\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "plt.plot(epochs[:len(train_losses)], train_losses, label='Training Loss')\n",
        "plt.plot(epochs[:len(eval_losses)], eval_losses, label='Validation Loss')\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "# plt.title('Training and Evaluation Metrics Across Epochs')\n",
        "plt.legend()\n",
        "# plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SfLEQNz-I1qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loghistory = pd.DataFrame(trainer.state.log_history)\n",
        "loghistory = loghistory.fillna(0)\n",
        "loghistory = loghistory.groupby(['epoch']).sum()\n",
        "fig, ax = plt.subplots(figsize=(6, 4))  # Set the size here (width, height)\n",
        "loghistory[[\"eval_accuracy\"]].plot(subplots=True, ax=ax)\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "ax.legend(handles=handles, labels=['Validation ACC'], loc='best')\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "OygY4awbI5YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention rollout**"
      ],
      "metadata": {
        "id": "JZKWHPHDKJrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# image_path_roll='1e08a369-c042-4f84-a9b7-ccec059aee77.jpg'\n",
        "image_path_roll = \"./10B02_CRC-Prim-HE-07_014.tif_Row_301_Col_2251.tif\"\n",
        "image_roll = Image.open(image_path_roll).convert(\"RGB\")\n",
        "inputs_roll = image_processor(images=image_roll, return_tensors=\"pt\").to(device)"
      ],
      "metadata": {
        "id": "C76sB-2zKO5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from typing import List, Callable, Optional"
      ],
      "metadata": {
        "id": "VQsr3hobKS4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_rollout(model, inputs):\n",
        "    discard_ratio=0.7\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs,output_attentions=True)\n",
        "        attentions = outputs.attentions  # Tuple of attention tensors for each layer\n",
        "\n",
        "    num_tokens = attentions[0].size(-1)\n",
        "    result = torch.eye(num_tokens).to(device)\n",
        "    ###########################\n",
        "    for attn in attentions:\n",
        "        attn_heads_fused = attn.mean(dim=1)  # Average over heads\n",
        "\n",
        "\n",
        "        # flat = attn_heads_fused.view(attn_heads_fused.size(0), -1)\n",
        "        # _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
        "        # indices = indices[indices != 0]\n",
        "        # flat[0, indices] = 0\n",
        "\n",
        "        attn_heads_fused += torch.eye(num_tokens).to(device)  # Add residual connection\n",
        "        attn_heads_fused /= attn_heads_fused.sum(dim=-1, keepdim=True)  # Normalize\n",
        "        result = torch.matmul(attn_heads_fused, result)\n",
        "\n",
        "    mask = result[0, 0, 1:]  # Discard CLS token\n",
        "    width = height = int(np.sqrt(mask.shape[0]))\n",
        "    mask = mask.reshape(height, width).cpu().numpy()\n",
        "    mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
        "    return mask\n",
        "\n"
      ],
      "metadata": {
        "id": "-WsjJvwuKZFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rollout_mask = get_attention_rollout(model, inputs_roll)\n",
        "rollout_mask_resized = Image.fromarray(np.uint8(rollout_mask * 255)).resize(image_roll.size, resample=Image.BILINEAR)\n",
        "rollout_mask_resized = np.array(rollout_mask_resized) / 255.0  # Normalize back to [0,1]\n",
        "\n",
        "# Convert image to numpy and normalize\n",
        "image_np = np.array(image_roll) / 255.0\n",
        "\n",
        "# Apply colormap to mask\n",
        "cmap = plt.get_cmap('jet')\n",
        "colored_mask = cmap(rollout_mask_resized)[..., :3]  # Drop alpha channel\n",
        "\n",
        "# Blend original image with attention mask\n",
        "alpha = 0.5\n",
        "blended = (1 - alpha) * image_np + alpha * colored_mask\n",
        "blended = np.clip(blended, 0, 1)\n",
        "\n",
        "# Display side-by-side\n",
        "plt.figure(figsize=(6, 3))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image_np)\n",
        "plt.axis('off')\n",
        "plt.title(\"Original Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(blended)\n",
        "plt.axis('off')\n",
        "plt.title(\"ViT Attention Rollout\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5snpE2VCKhUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average Drop**"
      ],
      "metadata": {
        "id": "ONZ3FC7xMY_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_average_drop2(model, image: Image.Image, rollout_mask: np.ndarray, processor):\n",
        "    device = model.device\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        predicted_class = probs.argmax(dim=-1)\n",
        "        original_conf = probs[0, predicted_class].item()\n",
        "\n",
        "    # Resize mask and apply to image\n",
        "    rollout_resized = cv2.resize(rollout_mask, (image.width, image.height))\n",
        "    rollout_resized = (rollout_resized - rollout_resized.min()) / (rollout_resized.max() - rollout_resized.min())\n",
        "    image_np = np.array(image).astype(np.float32) / 255.0\n",
        "    masked_image_np = image_np * rollout_resized[..., np.newaxis]\n",
        "    masked_image_pil = Image.fromarray((masked_image_np * 255).astype(np.uint8))\n",
        "\n",
        "    masked_inputs = processor(images=masked_image_pil, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        masked_logits = model(**masked_inputs).logits\n",
        "        masked_probs = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
        "        masked_conf = masked_probs[0, predicted_class].item()\n",
        "\n",
        "    avg_drop = max(0.0, (original_conf - masked_conf) / original_conf)\n",
        "\n",
        "    return avg_drop, original_conf, masked_conf, rollout_resized, masked_image_pil\n",
        "\n"
      ],
      "metadata": {
        "id": "A--KcxOkU8Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_average_drop_on_images(model, processor, image_paths):\n",
        "    avg_drops = []\n",
        "    for i, path in enumerate(image_paths):\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Attention rollout\n",
        "        rollout_mask = get_attention_rollout(model, inputs)\n",
        "\n",
        "        # Compute Average Drop + visuals\n",
        "        drop, orig, masked, mask_img, masked_pil = compute_average_drop2(model, image, rollout_mask, processor)\n",
        "        avg_drops.append(drop)\n",
        "\n",
        "        # Visualize\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "        axs[0].imshow(image)\n",
        "        axs[0].set_title(\"Original Image\")\n",
        "        axs[1].imshow(mask_img, cmap=\"jet\")\n",
        "        axs[1].set_title(\"Attention Rollout Mask\")\n",
        "        axs[2].imshow(masked_pil)\n",
        "        axs[2].set_title(f\"Masked Image\\nDrop: {drop:.3f}\")\n",
        "        for ax in axs: ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    print(f\"\\nðŸ“Š Average Drop across {len(image_paths)} images: {np.mean(avg_drops):.4f}\")\n",
        "    return avg_drops"
      ],
      "metadata": {
        "id": "Ahe8dFnZMXZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image_paths = [\"./kvasir_test_images/kvasir0.jpg\", \"./kvasir_test_images/kvasir1.jpg\", \"./kvasir_test_images/kvasir2.jpg\", \"./kvasir_test_images/kvasir3.jpg\", \"./kvasir_test_images/kvasir4.jpg\",\"./kvasir_test_images/kvasir5.jpg\",\"./kvasir_test_images/kvasir6.jpg\",\"./kvasir_test_images/kvasir7.jpg\"]"
      ],
      "metadata": {
        "id": "86Fm8v07nBfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [\"./colorectal_images/colorectal0.tif\", \"./colorectal_images/colorectal1.tif\", \"./colorectal_images/colorectal2.tif\", \"./colorectal_images/colorectal3.tif\", \"./colorectal_images/colorectal4.tif\",\"./colorectal_images/colorectal5.tif\",\"./colorectal_images/colorectal6.tif\",\"./colorectal_images/colorectal7.tif\"]\n",
        "evaluate_average_drop_on_images(model, image_processor, image_paths)"
      ],
      "metadata": {
        "id": "gqUPi_osMgkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**faithfulness**"
      ],
      "metadata": {
        "id": "yWFcwZzZPPwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.segmentation import slic\n",
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "id": "5acNvSN2MhNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_superpixels(image_np, n_segments=50):\n",
        "    # Segment image into superpixels\n",
        "    return slic(image_np, n_segments=n_segments, compactness=10)"
      ],
      "metadata": {
        "id": "5bDLEFUYPYWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_patch_attributions(mask, segments):\n",
        "    patch_scores = []\n",
        "    for label in np.unique(segments):\n",
        "        patch_mask = (segments == label).astype(np.float32)\n",
        "        score = np.sum(mask * patch_mask)\n",
        "        patch_scores.append(score)\n",
        "    return patch_scores\n"
      ],
      "metadata": {
        "id": "HQ71bV-lPZAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def faithfulness_eval_rollout_vit(model, image: Image.Image, processor,\n",
        "                                   baseline_color=(0, 0, 0), n_segments=50):\n",
        "    device = model.device\n",
        "    model.eval()\n",
        "\n",
        "    # Convert image to numpy array\n",
        "    image_np = np.array(image)\n",
        "    segments = get_superpixels(image_np, n_segments=n_segments)\n",
        "    inputs_roll = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    mask=get_attention_rollout(model, inputs_roll)\n",
        "\n",
        "    mask_resized = cv2.resize(mask, (image.width, image.height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    patch_attributions = compute_patch_attributions(mask_resized, segments)\n",
        "\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        predicted_class = probs.argmax(dim=-1)\n",
        "        original_conf = probs[0, predicted_class].item()\n",
        "\n",
        "    deltas = []\n",
        "    for patch_id in np.unique(segments):\n",
        "        img_masked = image_np.copy()\n",
        "        img_masked[segments == patch_id] = baseline_color\n",
        "        img_masked_pil = Image.fromarray(img_masked)\n",
        "\n",
        "        masked_inputs = processor(images=img_masked_pil, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            masked_logits = model(**masked_inputs).logits\n",
        "            masked_probs = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
        "            masked_conf = masked_probs[0, predicted_class].item()\n",
        "\n",
        "        delta_conf = original_conf - masked_conf\n",
        "        deltas.append(delta_conf)\n",
        "\n",
        "    correlation, _ = pearsonr(patch_attributions, deltas)\n",
        "    return correlation, patch_attributions, deltas, segments"
      ],
      "metadata": {
        "id": "iU0sNSLxPa5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in image_paths:\n",
        "  img = Image.open(i).convert(\"RGB\").resize((224, 224))\n",
        "  corr, scores, deltas, segs = faithfulness_eval_rollout_vit(model, img, image_processor)\n",
        "  print(\"Faithfulness (Pearson correlation):\", corr)\n"
      ],
      "metadata": {
        "id": "_LMHtRJiPgl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faithfulness_scores = []\n",
        "\n",
        "for i in image_paths:\n",
        "\n",
        "    img = Image.open(i).convert(\"RGB\").resize((224, 224))\n",
        "    corr, scores, deltas, segs = faithfulness_eval_rollout_vit(model, img, image_processor)\n",
        "    if np.std(scores) > 0 and np.std(deltas) > 0:\n",
        "        corr, _ = pearsonr(scores, deltas)\n",
        "        faithfulness_scores.append(corr)\n",
        "    else:\n",
        "        faithfulness_scores.append(0.0)\n"
      ],
      "metadata": {
        "id": "AloOH7ypPhTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_faithfulness = np.mean(faithfulness_scores)\n",
        "std_faithfulness = np.std(faithfulness_scores)\n",
        "\n",
        "print(f\"Faithfulness: Mean = {mean_faithfulness:.4f}, Std = {std_faithfulness:.4f}\")"
      ],
      "metadata": {
        "id": "N_Y5q5z6PlHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**"
      ],
      "metadata": {
        "id": "9YpM1xXySHQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_predict_label(model, inputs):\n",
        "  with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "  predicted_label = logits.argmax(-1).item()\n",
        "  model.config.id2label[predicted_label]\n",
        "  return model.config.id2label[predicted_label]"
      ],
      "metadata": {
        "id": "y1HPyvu4SF_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attention_batch(model, images, get_attention_rollout):\n",
        "    num_images = len(images)\n",
        "    fig, axes = plt.subplots(num_images, 3, figsize=(9, 3 * num_images))\n",
        "    if num_images == 1:\n",
        "        axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "    for i,img in enumerate(images):\n",
        "        image_pil = Image.open(img).convert(\"RGB\")\n",
        "        input_tensor = image_processor(images=image_pil, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # --- Attention Mask ---\n",
        "        rollout_mask = get_attention_rollout(model, input_tensor)  # (H, W), values in [0, 1]\n",
        "        resized_mask = Image.fromarray(np.uint8(rollout_mask * 255)).resize(image_pil.size, resample=Image.BILINEAR)\n",
        "        resized_mask = np.array(resized_mask) / 255.0\n",
        "\n",
        "        # --- Image & Mask Prep ---\n",
        "        image_np = np.array(image_pil) / 255.0\n",
        "        cmap = plt.get_cmap('jet')\n",
        "        heatmap = cmap(resized_mask)[..., :3]  # Drop alpha\n",
        "        overlay = np.clip((1 - 0.5) * image_np + 0.5 * heatmap, 0, 1)\n",
        "\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #     logits = model(input_tensor.unsqueeze(0))\n",
        "        #     pred_idx = logits.argmax(dim=1).item()\n",
        "        #     label = label_map[pred_idx] if label_map else str(pred_idx)\n",
        "        pred_label = model_predict_label(model, input_tensor)\n",
        "\n",
        "        # --- Plotting ---\n",
        "        axes[i, 0].imshow(image_np)\n",
        "        axes[i, 0].axis('off')\n",
        "        axes[i, 0].set_title(\"Original\")\n",
        "\n",
        "        axes[i, 1].imshow(heatmap)\n",
        "        axes[i, 1].axis('off')\n",
        "        axes[i, 1].set_title(\"Attention Heatmap\")\n",
        "\n",
        "        axes[i, 2].imshow(overlay)\n",
        "        axes[i, 2].axis('off')\n",
        "        axes[i, 2].set_title(f\"Pred: {pred_label}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sPoRomL-SKj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_attention_batch(model,image_paths, get_attention_rollout)"
      ],
      "metadata": {
        "id": "0kjhXvKhSUTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grad-CAM**"
      ],
      "metadata": {
        "id": "HzxaakDxSZ4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model wrapper to return a tensor\n",
        "class HuggingfaceToTensorModelWrapper(torch.nn.Module):\n",
        "  def __init__(self, model):\n",
        "    super(HuggingfaceToTensorModelWrapper, self).__init__()\n",
        "    self.model = model\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x).logits\n",
        "\n",
        "# Translate the category name to the category index\n",
        "def category_name_to_index(model, category_name):\n",
        "  name_to_index = {v: int(k) for k, v in model.config.id2label.items()}\n",
        "  return name_to_index[category_name]\n",
        "\n",
        "def add_caption_to_image(image, caption):\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 0.5\n",
        "    font_color = (255, 255, 255)  # White text\n",
        "    thickness = 2\n",
        "    # Add a black rectangle on top for better contrast\n",
        "    cv2.rectangle(image, (0, 0), (image.shape[1], 30), (0, 0, 0), -1)\n",
        "    # Add text\n",
        "    cv2.putText(image, caption, (10, 20), font, font_scale, font_color, thickness, cv2.LINE_AA)\n",
        "    return image\n",
        "\n",
        "# Helper function to run GradCAM on an image and create a visualization\n",
        "def run_grad_cam_on_image(model: torch.nn.Module,\n",
        "                          target_layer: torch.nn.Module,\n",
        "                          reshape_transform: Optional[Callable],\n",
        "                          input_tensor: torch.Tensor,\n",
        "                          input_image: Image,\n",
        "                          method: Callable = GradCAM):\n",
        "    # Get model prediction\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor.unsqueeze(0)).logits\n",
        "        predicted_class_idx = logits.argmax(dim=-1).item()\n",
        "\n",
        "    # Define Grad-CAM target as the predicted class\n",
        "    targets_for_gradcam = [ClassifierOutputTarget(predicted_class_idx)]\n",
        "\n",
        "    with method(model=HuggingfaceToTensorModelWrapper(model),\n",
        "                target_layers=[target_layer],\n",
        "                reshape_transform=reshape_transform) as cam:\n",
        "\n",
        "        grayscale_cam = cam(input_tensor=input_tensor.unsqueeze(0),\n",
        "                            targets=targets_for_gradcam)[0]\n",
        "\n",
        "        # Visualize Grad-CAM\n",
        "        visualization = show_cam_on_image(np.float32(input_image) / 255,\n",
        "                                          grayscale_cam,\n",
        "                                          use_rgb=True)\n",
        "\n",
        "        # Resize for easier display\n",
        "        visualization = cv2.resize(visualization, (input_image.width, input_image.height))\n",
        "\n",
        "\n",
        "        # Add predicted class name as caption\n",
        "        class_name = model.config.id2label[predicted_class_idx]\n",
        "        visualization = add_caption_to_image(visualization, class_name)\n",
        "\n",
        "        return visualization, grayscale_cam\n",
        "\n",
        "###########################################\n",
        "              # return np.hstack(results)\n",
        "\n",
        "# Define the reshape transform for ViT model\n",
        "def reshape_transform_vit_huggingface(x):\n",
        "  activations = x[:, 1:, :]\n",
        "  activations = activations.view(activations.shape[0],\n",
        "                                 14, 14, activations.shape[2])\n",
        "  activations = activations.transpose(2, 3).transpose(1, 2)\n",
        "  return activations\n",
        "\n",
        "# Define the target layer for GradCAM\n",
        "target_layer_gradcam = model.vit.encoder.layer[-2].output\n",
        "# Define the targets for GradCAM\n",
        "targets_for_gradcam = [ClassifierOutputTarget(category_name_to_index(model, 'ADIPOSE')),\n",
        "                       ClassifierOutputTarget(category_name_to_index(model, 'COMPLEX')),\n",
        "                       ClassifierOutputTarget(category_name_to_index(model, 'DEBRIS')),\n",
        "                       ClassifierOutputTarget(category_name_to_index(model, 'EMPTY')),\n",
        "                       ClassifierOutputTarget(category_name_to_index(model, 'LYMPHO')),\n",
        "                       ClassifierOutputTarget(category_name_to_index(model, 'MUCOSA')),\n",
        "                       ClassifierOutputTarget(category_name_to_index(model, 'STROMA')),\n",
        "                       ClassifierOutputTarget(category_name_to_index(model, 'TUMOR')),\n",
        "                      ]\n",
        "\n",
        "def process_images_with_grad_cam(images: List[str]):\n",
        "  results = []\n",
        "  gcamss=[]\n",
        "  for image_path in images:\n",
        "    # Load the input image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Resize the input image to match the model's expected input size (224x224)\n",
        "    image_resized = image.resize((224, 224))\n",
        "\n",
        "    # Convert the resized image into a tensor\n",
        "    tensor_resized = transforms.ToTensor()(image_resized)\n",
        "\n",
        "    # Run Grad-CAM on the resized image\n",
        "    visualization,gcam = run_grad_cam_on_image(model=model,\n",
        "                                          target_layer=target_layer_gradcam,\n",
        "\n",
        "                                          input_tensor=tensor_resized,\n",
        "                                          input_image=image_resized,\n",
        "                                          reshape_transform=reshape_transform_vit_huggingface)\n",
        "    results.append(visualization)\n",
        "    gcamss.append(gcam)\n",
        "  return results, gcamss\n",
        "\n",
        "\n",
        "image_paths_for_grad =  [\"./colorectal_images/colorectal0.tif\", \"./colorectal_images/colorectal1.tif\", \"./colorectal_images/colorectal2.tif\", \"./colorectal_images/colorectal3.tif\", \"./colorectal_images/colorectal4.tif\",\"./colorectal_images/colorectal5.tif\",\"./colorectal_images/colorectal6.tif\",\"./colorectal_images/colorectal7.tif\"]\n",
        "visualizations,grad_cams = process_images_with_grad_cam(images=image_paths_for_grad)\n",
        "\n",
        "\n",
        "\n",
        "print(grad_cams[0].shape)\n",
        "# Display the visualizations\n",
        "# for visualization in visualizations:\n",
        "#   display(Image.fromarray(visualization))\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "def show_images_grid(images, cols=3, figsize=(15, 10)):\n",
        "    rows = math.ceil(len(images) / cols)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    # Hide any leftover empty subplots\n",
        "    for j in range(i+1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "show_images_grid(visualizations, cols=3)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "11GOaPBdSXig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_cams[0]"
      ],
      "metadata": {
        "id": "GZV_s6ifTcuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap = np.uint8(255 * grad_cams[2])\n",
        "heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(heatmap_color)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RnH9wByPU2KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_average_drop_grad(model, processor, image_paths):\n",
        "    ioc_count = 0\n",
        "    avg_drops = []\n",
        "    for i, path in enumerate(image_paths):\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Attention rollout\n",
        "        grad_cam_mask = grad_cams[i]\n",
        "\n",
        "        # Compute Average Drop + visuals\n",
        "        drop, orig, masked, mask_img, masked_pil = compute_average_drop2(model, image, grad_cam_mask, processor)\n",
        "        print(\"orif confidence\",orig)\n",
        "        print(\"masked confidence\",masked)\n",
        "\n",
        "        # IOC\n",
        "        if masked > orig:\n",
        "          ioc_count += 1\n",
        "\n",
        "        avg_drops.append(drop)\n",
        "\n",
        "        # Visualize\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "        axs[0].imshow(image)\n",
        "        axs[0].set_title(\"Original Image\")\n",
        "        axs[1].imshow(mask_img, cmap=\"jet\")\n",
        "        axs[1].set_title(\"Attention Rollout Mask\")\n",
        "        axs[2].imshow(masked_pil)\n",
        "        axs[2].set_title(f\"Masked Image\\nDrop: {drop:.3f}\")\n",
        "        for ax in axs: ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    ioc_score = ioc_count / len(image_paths)\n",
        "    print(f\"IOC Score: {ioc_score}\")\n",
        "    print(f\"\\nðŸ“Š Average Drop across {len(image_paths)} images: {np.mean(avg_drops):.4f}\")\n",
        "    return avg_drops"
      ],
      "metadata": {
        "id": "6DAOotJyVJKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = [\"./colorectal_images/colorectal0.tif\", \"./colorectal_images/colorectal1.tif\", \"./colorectal_images/colorectal2.tif\", \"./colorectal_images/colorectal3.tif\", \"./colorectal_images/colorectal4.tif\",\"./colorectal_images/colorectal5.tif\",\"./colorectal_images/colorectal6.tif\",\"./colorectal_images/colorectal7.tif\"]\n",
        "evaluate_average_drop_grad(model, image_processor, image_paths)"
      ],
      "metadata": {
        "id": "Vc8bRrJGVc4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def faithfulness_eval_grad(model, image: Image.Image, processor,grad_cam_mask,\n",
        "                                   baseline_color=(0, 0, 0), n_segments=50):\n",
        "    device = model.device\n",
        "    model.eval()\n",
        "\n",
        "    # Convert image to numpy array\n",
        "    image_np = np.array(image)\n",
        "    segments = get_superpixels(image_np, n_segments=n_segments)\n",
        "    inputs_roll = image_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    mask=grad_cam_mask\n",
        "\n",
        "    mask_resized = cv2.resize(mask, (image.width, image.height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    patch_attributions = compute_patch_attributions(mask_resized, segments)\n",
        "\n",
        "    # Original prediction\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        predicted_class = probs.argmax(dim=-1)\n",
        "        original_conf = probs[0, predicted_class].item()\n",
        "\n",
        "    deltas = []\n",
        "    for patch_id in np.unique(segments):\n",
        "        img_masked = image_np.copy()\n",
        "        img_masked[segments == patch_id] = baseline_color\n",
        "        img_masked_pil = Image.fromarray(img_masked)\n",
        "\n",
        "        masked_inputs = processor(images=img_masked_pil, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            masked_logits = model(**masked_inputs).logits\n",
        "            masked_probs = torch.nn.functional.softmax(masked_logits, dim=-1)\n",
        "            masked_conf = masked_probs[0, predicted_class].item()\n",
        "\n",
        "        delta_conf = original_conf - masked_conf\n",
        "        deltas.append(delta_conf)\n",
        "\n",
        "    correlation, _ = pearsonr(patch_attributions, deltas)\n",
        "    return correlation, patch_attributions, deltas, segments"
      ],
      "metadata": {
        "id": "LTQW79BqVdvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "faithfulness_scores_grad = []\n",
        "\n",
        "for i,imgg in enumerate(image_paths):\n",
        "\n",
        "    img = Image.open(imgg).convert(\"RGB\").resize((224, 224))\n",
        "    corr, scores, deltas, segs = faithfulness_eval_grad(model, img, image_processor,grad_cams[i])\n",
        "    if np.std(scores) > 0 and np.std(deltas) > 0:\n",
        "        corr, _ = pearsonr(scores, deltas)\n",
        "        faithfulness_scores_grad.append(corr)\n",
        "    else:\n",
        "        faithfulness_scores_grad.append(0.0)"
      ],
      "metadata": {
        "id": "BeToBCf5Voc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_faithfulness_g = np.mean(faithfulness_scores_grad)\n",
        "std_faithfulness_g = np.std(faithfulness_scores_grad)\n",
        "\n",
        "print(f\"Faithfulness: Mean = {mean_faithfulness_g:.4f}, Std = {std_faithfulness_g:.4f}\")"
      ],
      "metadata": {
        "id": "PDWDpCFiVu8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_grad_batch(model, images, grad_cams):\n",
        "    num_images = len(images)\n",
        "    fig, axes = plt.subplots(num_images, 3, figsize=(9, 3 * num_images))\n",
        "    if num_images == 1:\n",
        "        axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "    for i,img in enumerate(images):\n",
        "        image_pil = Image.open(img).convert(\"RGB\")\n",
        "        input_tensor = image_processor(images=image_pil, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # --- Grad-CAM Mask ---\n",
        "        grad_mask = grad_cams[i]  # (H, W), values in [0, 1]\n",
        "        resized_mask = Image.fromarray(np.uint8(grad_mask * 255)).resize(image_pil.size, resample=Image.BILINEAR)\n",
        "        resized_mask = np.array(resized_mask) / 255.0\n",
        "\n",
        "        # --- Image & Mask Prep ---\n",
        "        image_np = np.array(image_pil) / 255.0\n",
        "        cmap = plt.get_cmap('jet')\n",
        "        heatmap = cmap(resized_mask)[..., :3]  # Drop alpha\n",
        "        overlay = np.clip((1 - 0.5) * image_np + 0.5 * heatmap, 0, 1)\n",
        "\n",
        "        # --- Prediction ---\n",
        "        # with torch.no_grad():\n",
        "        #     logits = model(input_tensor.unsqueeze(0))  # Add batch dim\n",
        "        #     pred_idx = logits.argmax(dim=1).item()\n",
        "        #     label = label_map[pred_idx] if label_map else str(pred_idx)\n",
        "        pred_label = model_predict_label(model, input_tensor)\n",
        "\n",
        "        # --- Plotting ---\n",
        "        axes[i, 0].imshow(image_np)\n",
        "        axes[i, 0].axis('off')\n",
        "        axes[i, 0].set_title(\"Original\")\n",
        "\n",
        "        axes[i, 1].imshow(heatmap)\n",
        "        axes[i, 1].axis('off')\n",
        "        axes[i, 1].set_title(\"Grad-CAM Heatmap\")\n",
        "\n",
        "        axes[i, 2].imshow(overlay)\n",
        "        axes[i, 2].axis('off')\n",
        "        axes[i, 2].set_title(f\"Pred: {pred_label}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "F7_rjA7DX18X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_grad_batch(model, image_paths, grad_cams)"
      ],
      "metadata": {
        "id": "YmXqTDysYH88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sw1Rvk7xYLfu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}